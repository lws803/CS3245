{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Construction\n",
    "\n",
    "Can use BSBI, SPIMI, and distributed indexing to make index construction more scalable.\n",
    "\n",
    "## Hardware Limitations and Bottlenecks\n",
    "`Seek time` - time a mechanical hand takes to move to a random location in the disk\n",
    "\n",
    "`Transfer time` - time to transfer a data block\n",
    "\n",
    "- Access to data in memory is *much* faster than acces to data on disk. \n",
    "- No data is transferred during disk seeks, hence transferring *large chunks* of data from disk to memory is faster than transferring many small chunks. \n",
    "- Disk I/O is block based\n",
    "\n",
    "\n",
    "\n",
    "`s` - Average seek time, is on average *8 ms*\n",
    "\n",
    "`b` - Transfer time per byte, is on average *0.006 Î¼s*\n",
    "\n",
    "- Dataset used is [Reteurs RCV1](https://archive.ics.uci.edu/ml/datasets/Reuters+RCV1+RCV2+Multilingual,+Multiview+Text+Categorization+Test+collection)\n",
    "- It is a newswire collection from 1995 and 1996\n",
    "- 800,000 documents; 100,000,000 non-positional postings; avg. bytes per token: 4.5; avg. bytes per term: 7.5\n",
    "- Average no. of bytes per token is lower because of stop word removal. Many tokens may actually be stop words, and the removal of them will lead to the average size per token being lower.\n",
    "\n",
    "## Blocked Sort-Based Indexing\n",
    "- Idea is to sort with fewer disk seeks\n",
    "- 8-byte (4+4) records (termID, docID)\n",
    "- We now sort 8-byte records by termID (termID used instead of the term to increase speed)\n",
    "- Basic idea: accumulate postings for each *block*, sort, then write to disk\n",
    "- Then merge the blocks into one long sorted order\n",
    "\n",
    "### Sorting example\n",
    "In order to sort 10 blocks of 10 million records:\n",
    "\n",
    "1. Accumulate entries for a block, sort within and write to this disk:\n",
    "2. Quicksort takes *N* in *N* expected steps (in our case 10 million in 10 million steps)\n",
    "3. 10 times this estimate - 10 sorted *runs* of 10 million records each on disk\n",
    "\n",
    "### Merging the blocks\n",
    "First approach: binary merge with a merge tree. During each layer, read into memory runs in blocks of 10 million, merge, then write back. \n",
    "\n",
    "Second approach (n-way merge):\n",
    "1. Reading from all blocks simultaneously\n",
    "2. Read decent-sized chunks of each block into memory, merge, then write out to decent sized output chunk. Efficiency isn't lost by disk seeks\n",
    "\n",
    "## Single-Pass in-memory indexing\n",
    "- Idea #1: Generate separate dictionaries for each block - no need to maintain term-termID mapping across blocks. \n",
    "- Idea #2: Build the \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
