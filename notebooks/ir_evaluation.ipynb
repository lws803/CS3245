{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Evaluation\n",
    "- To do system evaluation, we require an overt expression of an information eed\n",
    "- For relevance we choose to use a binary decision of relevance\n",
    "- Precision = true positive/(all retrieved docs)\n",
    "- Recall = true positive/(all relevant docs, include those not retrieved)\n",
    "\n",
    "## Unranked retrieval evaluation\n",
    "\n",
    "### Accuracy\n",
    "`(tp+tn)/all_docs`\n",
    "\n",
    "- However, there is a way to obtain high accuracy as most documents (99%) are not relevant. A system can obtain high accuracy by declaring every document not relevant. And if we label some documents as relevant, there will be alot of false positives.\n",
    "\n",
    "### F measure (harmonic mean)\n",
    "Balanced method of evaluating a system's precision and recall and retrieval.\n",
    "\n",
    "- Can tune beta and alpha to achieve balance `(higher beta -> smaller alpha -> favours recall)`\n",
    "- Beta > 1 favours recall, beta < 1 emphasises precision\n",
    "\n",
    "#### Pros over accuracy measure\n",
    "- Professional searchers (eg, paralegals) will favour higher recall instead as irrelevant docs may be useful\n",
    "\n",
    "#### Pros over arithmetic mean:\n",
    "- Arithmetic mean is evenly distributed, meaning a 100% recall (returning all docs) and yield 50% of the average mean\n",
    "\n",
    "### F measure for classes\n",
    "\n",
    "eg. given a fixed number of classes: (person_name, company_name, neither).\n",
    "\n",
    "We wish to find the F measure of the prediction on these classes. To do so we have to use micro/ macro averaging.\n",
    "\n",
    "- Use micro averaging when we wish to determine the effectiveness in large classes\n",
    "- Use macro averaging when we wish to determine effectiveness of small classes  \n",
    "\n",
    "**Microaveraging** pools per document decisions across classes and consider the F measure there.  \n",
    "**Macroaveraging** calculate F measure per class and then average them.\n",
    "\n",
    "## Evaluation of ranked results\n",
    "\n",
    "###  Precision recall curve\n",
    "- System can return any number of results\n",
    "- We take various numbers of the top returned documents (different levels of recall)\n",
    "\n",
    "### Precision at top K (widely used)\n",
    "Find precision at top K results. Good for most web searches as people only want the good matches on the first or second result page.\n",
    "\n",
    "- Does not average well as it depends greatly on the number of relevant docs for a query\n",
    "\n",
    "### 11-point interpolated average precision\n",
    "Take precision at 11 levels of recall varying from 0 to 1 by tenths of documents using interpolation and average them. This evaluates performance at all recall levels.\n",
    "\n",
    "\n",
    "### MAP (mean average precision)\n",
    "Area under precision-recall curve (uninterpolated)\n",
    "\n",
    "- Dependent on the test collection - arithmetic average of the query collection, some can get MAP of 0.7 and some 0.1\n",
    "- MAP treats every information need with equal weightage\n",
    "- MAP scores vary from user to user (information need), thus it is important to have multiple query categories\n",
    "\n",
    "### R-precision\n",
    "With known set of relevant documents size `Rel`, calculate the precision of top `Rel` docs returned  \n",
    "If there are `Rel` documents for a query and we find that `r` are relevant, we find R-precision by taking `r/Rel`\n",
    "\n",
    "# Creating test collections\n",
    "\n",
    "We need:\n",
    "\n",
    "- test queries (what queries to choose which are relevant to the docs available)\n",
    "- relevance assessments (what is relevant?)\n",
    "\n",
    "### Kappa measure\n",
    "\n",
    "- Agreement measure among judges\n",
    "- Designed for categorical judgments\n",
    "- Corrects for chance agreement\n",
    "\n",
    "`K = (P(A)-P(E))/(1-P(E))`\n",
    "\n",
    "P(A): proportion of time judges agree  \n",
    "P(E): Probability of chance agreement = P(non-relevant)^2 + P(relevant)^2  \n",
    "\n",
    "Some examples:\n",
    "\n",
    "- Kappa > 0.8 good agreement\n",
    "- 0.67 < Kappa < 0.8 fair agreement\n",
    "- Kappa < 0.67 dubious\n",
    "\n",
    "### Other factors\n",
    "\n",
    "Relevance vs marginal relevance\n",
    "- A document can be redundant even if it is highly relevant eg. useless spam\n",
    "- Duplicates\n",
    "- Same info from different sources (git forks)?\n",
    "- Marginal relevance is a better measure of utlity for the user\n",
    "\n",
    "### Other methods\n",
    "\n",
    "- Use crowdsourcing methods to collect data\n",
    "- A/B testing\n",
    "    - evaluate with automatic overall evaluation criterion (OEC) eg. clickthrough on first result\n",
    "    - However, still have pitfalls when both A and B versions are vastly different, hard to associate OEC with changes\n",
    "- In lab testing, study user behaviours in the lab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
