{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens and terms\n",
    "\n",
    "Common complications in extracting text:\n",
    "- single documents may have multiple languages (eg. French email with German PDF)\n",
    "- What is the unit document? (a group of files? a single file?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation\n",
    "\n",
    "Simple breaking up of a sentence based on a few delimiters.\n",
    "\n",
    "## Issues and types of delims\n",
    "1. Apostrophes (eg. `Finland's capital`) do we remove the apostrophe?\n",
    "2. Hyphens (eg. `state-of-the-art`) break up the hyphenated sequence\n",
    "3. Spaces\n",
    "This is not a one size fits all solution as we need to know the use case.\n",
    "\n",
    "\n",
    "##  Numbers, dates and other dangerous things\n",
    "\n",
    "How do we try to process dates like these:\n",
    "\n",
    "```\n",
    "3/20/13\n",
    "55 B.C.\n",
    "B-52\n",
    "```\n",
    "\n",
    "## Language issue\n",
    "\n",
    "- French has words such as `L'ensemble` this is supposed to match with `un ensemble`\n",
    "- German noun compounds are not segmented - can be a very long combined sentence with no spaces.\n",
    "- Japanese and Chinese has no spaces between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "print ('hello world'.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words\n",
    "\n",
    "With a **stop list** we can exclude the most common words from dictionary. eg. `a`, `be`, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "  \n",
    "stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "word_tokens = word_tokenize(example_sent) \n",
    "  \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "filtered_sentence = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "  \n",
    "print(filtered_sentence) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation\n",
    "\n",
    "We would usually normalise words so that we don't have to store too much data.\n",
    "- Deleting periods: U.S.A -> USA\n",
    "- Deleting hyphens: anti-discriminatory -> antidiscriminatory\n",
    "- Removal of accents and umlauts\n",
    "\n",
    "# Case folding\n",
    "\n",
    "Reduce all letters to lower case.  \n",
    "Often better to reduce to lower case as user's inputs are usually in lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation\n",
    "\n",
    "Reduce inflectionla/ variant forms to base form.  \n",
    "eg.\n",
    "- am, she, is -> be\n",
    "- car, cars, car's, cars' -> car\n",
    "\n",
    "Lemmatisation implies doing *proper* reduction to dictionary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "wordnet_lemmatizer.lemmatize(\"dogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Reduce terms to their \"roots\" before indexing.  \n",
    "Stemming suggest crude affix chopping eg. `automate, automata, automation -> automat`\n",
    "\n",
    "## Porter's algorithm\n",
    "\n",
    "### Typical rules\n",
    "- sess -> ss\n",
    "- ies -> i\n",
    "- ational -> ate\n",
    "- tional -> tion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autom'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "porter_stemmer.stem(\"automation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "## Rank of techniques which will help reduce vocab size\n",
    "1. Stop words - a document is usually made up of a lot of stopwords\n",
    "2. Lemmatisation/ Stemming - a lot of words will be shortened to base forms\n",
    "3. Case folding - worst as a lot of permutation for words regardless if its just lower case only.\n",
    "\n",
    "\n",
    "## Do stemming and other normalisations help?\n",
    "- Harms precision for some queries in English but very helpful for some - mixed results\n",
    "- Definitely useful for Spanish, German, Finnish.\n",
    "\n",
    "# Ultimately\n",
    "1. Language-specific\n",
    "2. Application-specific"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
