{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram method for counting probabilities\n",
    "\n",
    "## Differences b/w unigram method and true ngram method\n",
    "1. Unigram method uses AND property of probability: just multiply the probabilities of words\n",
    "2. True NGram method uses conditional probability `P(abcd) = P(d|abc) * P(abc)`  \n",
    "This is how we obtain the probability by using a chain of conditional probabilities in **bigram**  \n",
    "`P('There was heavy rain') ~ P('There')P('was'|'There')P('heavy'|'There was')P('rain'|'There was heavy')`  \n",
    "Unfortunately there is no way to obtain the probability for `P(‘rain’|‘There was’)` using just **bigram** models.  \n",
    "3. True NGram will output very discreet values especially for `P(d|abc)` as `Freq(abcd)` could be very little, close to 0 or 1.\n",
    "4. Use the true NGram method only when we have enough data sets so that we yield more observations and more counts of the true Ngram will be stored.\n",
    "\n",
    "\n",
    "## Why do we use add-one smoothing\n",
    "To distribute some probability from seen words to unseen words. We pretend that everything is seen at least once. However, add-one smoothing gives a false impression of the index we create (hence why we don't perform add-k smoothing for instance).\n",
    "  \n",
    "If language model is constructed from large number of observations\n",
    "#### If small vocab but large observations\n",
    "This case would be a good choice for add one smoothing. With many observations but a small vocab, only a small probability mass will be distributed towards the unseen words.  \n",
    "eg. unseen words will get a probability of 1/(a very large number)\n",
    "\n",
    "#### If large vocab but small observations\n",
    "\n",
    "That could shift the distribution quite drastically towards the unseen words.  \n",
    "eg. if we use the english characters without add one smoothing and \"a\" was seen, then we will get P(a) = 1. But if we apply add one smoothing, then it will become P(a) = 1/27.\n",
    "\n",
    "\n",
    "### Add one or add-k smoothing taps on the belief that we should normalise and believe in all events (in the training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_collection_PC = [\"I Don't Want To Go\", \"A Groovy Kind Of Love\", \"You Can't Hurry Love\", \"This Must Be Love\", \"Take Me With You\"]\n",
    "text_collection_AS = [\"All Out Of Love\", \"Here I Am\", \"I Remember Love\", \"Love Is All\", \"Don't Tell Me\"]\n",
    "\n",
    "vocab = {} # We store vocab here to count the universal number of different vocabs available****\n",
    "text_collection_PC_freq = {}\n",
    "text_collection_AS_freq = {}\n",
    "total_text_collection_PC_freq = 0\n",
    "total_text_collection_AS_freq = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare our variabels and song titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formula for counting the probability\n",
    "\n",
    "`freq(word)/(total_freq_of_intended_dictionary + length(vocab))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settle the Air Supply frequency first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for titles in text_collection_AS:\n",
    "    for word in titles.split():\n",
    "        vocab[word] = 1\n",
    "        total_text_collection_AS_freq += 1\n",
    "        if (word not in text_collection_AS_freq):\n",
    "            text_collection_AS_freq[word] = 1\n",
    "        else:\n",
    "            text_collection_AS_freq[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settle the Phil Collins frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for titles in text_collection_PC:\n",
    "    for word in titles.split():\n",
    "        vocab[word] = 1\n",
    "        total_text_collection_PC_freq += 1\n",
    "\n",
    "        if (word not in text_collection_PC_freq):\n",
    "            text_collection_PC_freq[word] = 1\n",
    "        else:\n",
    "            text_collection_PC_freq[word] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "Total for AS, including total vocab size:  42\n",
      "Total for PC, including total vocab size:  48\n",
      "==========================\n"
     ]
    }
   ],
   "source": [
    "print (len(vocab))\n",
    "print (\"Total for AS, including total vocab size: \", (total_text_collection_AS_freq + len(vocab)))\n",
    "print (\"Total for PC, including total vocab size: \", (total_text_collection_PC_freq + len(vocab)))\n",
    "print (\"==========================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the query text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I Remember You\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate probability\n",
    "\n",
    "- We ignore the words that are not spotted in the dictionary\n",
    "- We also use the log space as probabilities can get really really small (this prevents underflow of floating point values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Air Supply -4.0915966208100585\n",
      "Phil Collins -4.265572461743118\n"
     ]
    }
   ],
   "source": [
    "probability_AS = 0\n",
    "probability_PC = 0\n",
    "\n",
    "for word in query.split():\n",
    "    if (word in text_collection_PC_freq):\n",
    "        # Remember to add one smoothing here\n",
    "        probability_PC += math.log10(text_collection_PC_freq[word] + 1) - math.log10(total_text_collection_PC_freq + len(vocab))\n",
    "    elif (word in vocab):\n",
    "        # Remember to consider as 1 if it does not exist in the dictionary but in the vocab\n",
    "        probability_PC += math.log10(1) - math.log10(total_text_collection_PC_freq + len(vocab))\n",
    "    else:\n",
    "        # If the word is unseen in the data in general, then we don't consider it at all\n",
    "        pass\n",
    "\n",
    "    \n",
    "for word in query.split():\n",
    "    if (word in text_collection_AS_freq):\n",
    "        # Remember to add one smoothing here\n",
    "        probability_AS += math.log10(text_collection_AS_freq[word] + 1) - math.log10(total_text_collection_AS_freq + len(vocab))\n",
    "    elif (word in vocab):\n",
    "        # Remember to consider as 1 if it does not exist in the dictionary but in the vocab\n",
    "        probability_AS += math.log10(1) - math.log10(total_text_collection_AS_freq + len(vocab))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print (\"Air Supply\", probability_AS)\n",
    "print (\"Phil Collins\", probability_PC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This title is from Air Supply\n"
     ]
    }
   ],
   "source": [
    "if (probability_PC < probability_AS):\n",
    "    print (\"This title is from Air Supply\")\n",
    "else:\n",
    "    print (\"This title is from Phil Collins\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
