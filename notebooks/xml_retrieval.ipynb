{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML retrieval\n",
    "\n",
    "### General idea\n",
    "\n",
    "Create a tree where each node contains the structure eg. author, title, citation...  \n",
    "each leaf contains the text. eg. `author -> surname -> \"penyet\"`\n",
    "\n",
    "### Lexicalised sub trees\n",
    "\n",
    "- Take each leaf node and break it into multiple nodes with its parents (representing the structure)\n",
    "\n",
    "Use a vector space to encode each word together with its position in the XML tree.  \n",
    "eg. `title -> \"microsoft\"` and `book -> author -> \"bill\"`\n",
    "\n",
    "**Difference between traditional VSMs:**  \n",
    "In this case we are storing the structure and comparing the structural similarity instead.\n",
    "\n",
    "#### Tradeoffs\n",
    "\n",
    "There is a tradeoff between dimensionality of space and accuracy of query results.\n",
    "- If we restrict dimensions to vocab terms, VSM will retrieve many documents that do not match the structure of query\n",
    "- If we create a seperate dimension for each **lexicalised subtree**, the dimensionality becomes too large\n",
    "\n",
    "Compromise: index all paths that end in a single vocab term. -> XML context term pairs. `<c, t>, c is the context tree`\n",
    "\n",
    "### Context resemblance\n",
    "\n",
    "Context resemblance is the measure of similarity between a path C_q of query and C_d of document. C_q matches C_d iff we can transform C_q into C_d by *inserting additional nodes*. (it does not take into account for transposition).  \n",
    "eg.  \n",
    "`author -> surname -> penyet` vs `author -> middlename -> ayam` do not match and will given a default score of 0.\n",
    "\n",
    "\n",
    "#### Cons of such resemblance\n",
    "1. Does not consider transposition, eg. what if we wish to compare between surname vs middlename.\n",
    "2. Users need to know the exact structural types within the system to know what structural type to use  \n",
    "eg. user might wish to search for `author -> name -> wilson` instead of `author -> middlename -> wilson` in this case it wont match at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document similarity measure pseudo code\n",
    "\n",
    "def SimNoMerge (docs, q):\n",
    "    score = {}\n",
    "    for doc in docs:\n",
    "        score[doc] = 0\n",
    "    for c_q, term in q:\n",
    "        weight_q = weight (q, term, c_q) # Find the weight of term in XML structure in query\n",
    "        for context in XML_contexts:\n",
    "            # Note that c_q != context. c_q could be c_1 and we are comparing between\n",
    "            # c_1 vs c_2, c_1 vs c_3, c_1 vs c_4 etc etc\n",
    "            if (cr(c_q , context) > 0): # There is a match in lexicalised subtree\n",
    "                postings = get_postings((context, term))\n",
    "                for doc in postings:\n",
    "                    x = cr(c_q, context) * weight_q * weight(doc, term, context) \n",
    "                    # Weight of term in XML structure in document\n",
    "                    score[doc] += x\n",
    "    for doc in score:\n",
    "        score[doc] /= normaliser(doc)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML IR evaluation\n",
    "\n",
    "Initiative for the eval of XML retrieval. (INEX)\n",
    "\n",
    "#### Component coverage\n",
    "1. Exact coverage (E): information sought is the main topic of the component and the component is a meaningful unit of info\n",
    "2. Too small (S): the information sought is the main topic of the component but the component is not meaningful.\n",
    "3. Too large (L): the information sought is present in the component but is not the main topic\n",
    "4. No coverage (N): The information sought is not the topic of the component\n",
    "\n",
    "#### Topical relevance\n",
    "1. Highly relevant (3)\n",
    "2. Fairly relevant (2)\n",
    "3. Marginally relevant (1)\n",
    "4. Non relevant (0)\n",
    "\n",
    "#### Combination\n",
    "Relevance is judged based on the 2 components: relevance and coverage  \n",
    "We then use a quantisation function **Q** to grade each component as partially relevant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
