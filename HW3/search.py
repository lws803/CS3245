#!/usr/bin/python
import re
import nltk
import sys
import getopt
import os
import struct
import math
import string
import numpy as np
from numpy import linalg as LA
from collections import Counter


from nltk.stem.porter import PorterStemmer

# Parameters 
BYTE_WIDTH = 4 # Used in the unpacking of the postings list
IGNORE_PUNCTUATION = True # Adding an option to strip away all punctuation in a term
PERFORM_STEMMING = True # Adding an option to perform stemming on the current term

def usage():
    print ("usage: " + sys.argv[0] + " -d dictionary-file -p postings-file -q file-of-queries -o output-file-of-results")

dictionary_file = postings_file = file_of_queries = output_file_of_results = None
	
try:
    opts, args = getopt.getopt(sys.argv[1:], 'd:p:q:o:')
except (getopt.GetoptError):
    usage()
    sys.exit(2)

for o, a in opts:
    if o == '-d':
        dictionary_file  = a
    elif o == '-p':
        postings_file = a
    elif o == '-q':
        file_of_queries = a
    elif o == '-o':
        file_of_output = a
    else:
        assert False, "unhandled option"

if dictionary_file == None or postings_file == None or file_of_queries == None or file_of_output == None :
    usage()
    sys.exit(2)

# Global variables that represent the input files, as well as global lists such as the universe list
dictionary = None
output = None
postings = None
queries = None
operators = {"AND": 4, "OR": 3}
stemmer = PorterStemmer()
df = {} # Document frequencies for each term
universe = {} # Universal set of documents



# Class of logical functions processed in each query
class Logic:
    def cosineSimilarity (query):
        tf_q = Counter(nltk.word_tokenize(query))

        
        # For file among all files
        # Find the cosine similarity with the query term


        return []


def generateSkipList (data=[]):
    pass




def retriever (token):
    pass

# normaliseTerm user search query terms
def normaliseTerm (token):
    token = token.lower() # Perform case folding on the current term

   # Remove all instances of punctuation if the bool is set to true.
    if IGNORE_PUNCTUATION: # Remove all instances of punctuation
        token = token.translate(str.maketrans('','',string.punctuation))
            
    # Perform stemming on the term
    if PERFORM_STEMMING: token = stemmer.stem(token)
    
    return token



# Initialise the dictionary and universe set for all documents
def populateDictionaryAndUniverse (lines):
    firstLine = True
    for line in lines:
        if (firstLine):
            for ids in line.split(','):
                if (ids == ' \n'):
                    break
                universe[int(ids)] = True
            firstLine = False
        else:
            df[line.split()[0]] = (int(line.split()[1]), int(line.split()[2]))

# Process each query using the stack expression generated by the shunting yard algorithm
def query (lines):
    for line in lines:
        pass



if __name__ == "__main__":
    dictionary = open(dictionary_file, 'r')
    output = open(file_of_output, 'w')
    postings = os.open(postings_file, os.O_RDONLY)
    queries = open(file_of_queries, 'r')

    # Store terms in memory with their frequencies and starting byte offsets
    populateDictionaryAndUniverse(dictionary.readlines())

    # Process each query in the queries file
    query(queries.readlines())

    queries.close()
    dictionary.close()
    output.close()
    os.close(postings)
